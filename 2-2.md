# Februrary - Week 2

## Progress

- set up build environment
- realized build environment is very non-portable

### Plans

- define our own trap entry so we are not relying on compiler
  - mimic riscv-tests in how they do this
  - nested interrupt stuff -- make sure that interrupts are disabled while we
    are pushing/popping interrupts from the stack
- for me, rewire the entirity of boot... 
  - delete the part of `vm_boot` that Will told me to delete

Eileen 
- creating simple pre-loaded scheduler framework thing 

Henry 
- ecall framewor

Amber
- boot rewrite with TB core, linker, etc

### Realizations for the Far Future

- because of the way `riscvtests` is wired to the simulation, if we want to
  have the primary environment mimic the verification tests, we'll have to
  completely rewrite the library (eg `string.c`) because the way it outputs to
  `tohost` is completely different then the way the in-house tests output to
  `tohost`

## Questions to ask

General
- how does stuff like add.S test the actual virtual memory implementation?

Cole
- what did u hack into riscvtests lol
- why use `attribute((interrupt))` for the verification tests if that's
  compiler-dependent, rather than defining your own trap entry handler so you
  have more control 
- `attribute((interrupt))` only seems to handle mret within the compiler, rather
  than sret -- how did you account for that

Will
- format of testbench
- how tohost works

## 2/10

### TB format

`RISCVBusiness/tb_core.cc`

This is the standin for the core interfacing with the rest of the chip
(AFT-dev).

- verilator compiles the system verilog into cpp
- it object-orients the DUT (the core) so that it can alter what's going inside
  the DUT--for example, editing mtime, wdata, addr inside the dut by doing
  dut.addr
- within the RVB core's address space, there's `tohost`, which is used to
  write/read to the testbench. Tohost is cacheable, which is why it needs to be
  fenced.

What Cole did to `riscvtests`

- Cole hacked the testbench core to read the elf of the bin path to teh test
  bench in `riscvtests`, which is why the .tohost region exists in the
  `riscvtests` linker

Verification tests vs `riscvtests`
- `.tohost` is defined as `0x80001040` in `riscv_tests` environment
- `.tohost` is defined as `0` in the verification environment
- Within the verification tests:
  - when start.S jumps to done, it loads the flag into `0x28` register, which
    is the register used to communicate with the testbench
  - in `tb_core.cc`, line 424, we can see that it compares the flag value
    (expected 1 for a pass) using `!use_tohost && dut.top_core->get_x28() == 1`
  - this is also why we dont need `.tohost` to be the actual tohost address in
    the verification tests; it's not used

Printing in simulation 
- `MAGIC_ADDR` is `0xFFFFFFFC` and is defined in both `tb_core` and
  `utility.h`. The print library is wired to print to that address in the
  verification tests.
- Line `152` in `tb_core.cc` is where the testbench checks to writes at that
  address, and if writes exist, it prints to stdout.
- In `riscvtests`, print (putc) writes to `tohost` in a different system.

### Trap format

In `riscvtests`, a custom trap entry format is specified
- this is not the case with the verification tests, which use
  `__attribute((interrupt))` and rely on the compiler
- it may not be the best choice to rely on the compiler, so we will do like the
  riscv-tests do

Flow of control:
1) trap_entry
2) jump to trap handlera delegator
3) jump to correct trap handler based on scause

### Linker Script

ENTRY
- `rvt`: `_start`
- `ver`: `boot`
- start here to start editing differences

MEMORY START
- same place `0x80000000`
- use the `ver` of this

.TOHOST
- defined as a specific section of memory in `rvt`
- can probably nerf; or at least move to `0xFFFFFFFC` because that becomes I/O
  for the verification tests

ALIGN in `rvt`
- tohost, .text, and .text.init are both aligned to `0x1000`
- align moves the location ctr fowards to the next multiple of 4096 (page size)

TEXT
- `rvt`: contains .text.init and .text
- `ver`: contains .text.boot and .text
- seems that the only stuff in start.S / entry.S is .init/.boot; figure out how
  the rest of the .binfile programs are linked into .text (if at all)

DATA
- `rvt`: everythign in .data
- `ver`: .data, .rodata (i doubt this is actually read-only, it's probably
  writeable), .sdata (?), .eh_frame

BSS
- `rvt`: .bss
- `ver`: .sbss and .bss

what's preinit_arr and init_arr for? 
- both fucntiosn are arrays of function ptrs that the startup code calls before
  main() 
- libc bootstrap logic, hooks, c global initialization code (eg putting .data
  where it needs to be put)
- if we have pure assembly code, the startup code just skips them and continues
  to main. if \__init_array_start == \__init_array_end, the init array loop
  never runs.
  - no object file will ever provide the .init_array (?) 

currently, `rvt`: 
- .text.init starts at `RAM`
- .tohost starts at new 4KB page
- .text starts at new 4KB page
- .data starts o na new 4KB page

I think I just need to make sure that things align well and each section is
page-separated

## 2/11

Cleared vm.boot stuff and finished linker script

## 2/13

### Working on vm.boot.

page table notes from GT video example:
- https://www.youtube.com/watch?v=8kBPRrHOTwg

example of a small 2-level page table: 
- say page # is 4 bits; offset is 4 bits

using a flat page table:
- we'd need 2^4 entries: 0... 15
- we index using the page table and get our frame number 

now given a two-level page table
- we have a 4-bit page number split into 2 pieces, each are 2 bits
- we use the first number to index into an outer page table
- use the oute rpage table to give us the 2nd-level page table, and use the 2nd number to index into the 2nd page table to give our frame number
- where r the savings though? we have 4 entries in the outer page table, and 4 additional 4-size page tables, so we dont save space
- the savings r that -- if there is an unused large part of the addr space -- we can completley nerf one of the innner page tables and mark in the outer table that the entry isnt neccesary

virthual addresses in sv32, thank u chat

```
31          22 21          12 11          0
+-------------+-------------+-------------+
|   VPN[1]    |   VPN[0]    |   offset    |
+-------------+-------------+-------------+
    10 bits       10 bits        12 bits
```
- offset: byte offset inside the 4kb page
- vpn[0]: index into the l2 page table
- vpn[1]: intex into the l1 page table

in a 2-level walk:
1. use vpn[1] to index into l1
2. the entry in l1 points to a l2 page table
3. use vpn[0] to index into l2

what does the PTE store? 
- in SV32, page table entry is :
a) lower 10 bits: flags 
b) upper 22 bits: physical page number. NOT the full phywical address
- the offset (taken from the virtual addr, I beleive?) is added onto the PPN. so: 
  - physical address = (PTE.PPN << 12) | page_offset
  - because pages are 4KB, we have a 12-bit offset

Virtual memory is set up here:
```
// l1pt is pt[0]
l1pt[PTES_PER_PT-1] =
  (DRAM_BASE/RISCV_PGSIZE << PTE_PPN_SHIFT)
  | PTE_V | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D;
```
- sv32: 2-level page table, with each table having 1024 entries. 
- PTES_PER_PT = 1024, because ther are 1024 page tables per page table
- each L1 entry covers 4MBZ of virtual addr space, because there are 1024
  entries in L2 with each mapping to 4KB, so each L1 entry = 1024 * 4KB = 4MB. 
- l1pt[1023] is the last entry, which mapps the top MB of the virutal addres
  sspace. 
- DRAM_BASE/RISCV_PGSIZE converts physical addr to a physical page number? 
  - pg size is 2^12, so doing DRAMBASE/RISCVPAGESIZE just vies the physical page number
  - and then we << PTE_PPN_SHIFT because we wnat to shift to have room for the flags inside of the register itself.

pa2kva
- pa2kva: converts physical address into kernel virtual address (KVA) 
- dose this by subtracting `DRAM_BASE` (base physical address of RAM from
  `MEGAPAGE_SIZE`: the size of one large page

### VM boot notes restart because i am confused

sv32 virutal address:

```
[ VPN[1] (10 bits) | VPN[0] (10 bits) | offset (12 bits) ]
```
- so: each l1 index (vpn[1]) selects from the first 10 bits
- each l2 entry (vpn[i])) is the next 10 bits

`l1pt[0] = ((pte_t)user_l2pt >> PGSHIFT << PTE_PPN_SHIFT) | PTE_V;`
- maps user space to the l1 entry 0
- this will get got w/ vrtual addresses 0x00000000 – 0x003FFFFF
- the first 10 0's coem from the first 10 bits of the virtual addr
- any virtual addrs beyond these will produce page fualts I believe

```
l1pt[PTES_PER_PT-1] =
    (DRAM_BASE/RISCV_PGSIZE << PTE_PPN_SHIFT)
    | PTE_V | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D;
```
- maps kernel (high-level virtual memory)
- covers 0xFFC00000 – 0xFFFFFFFF (i htink)
- maps directly to DRAM_BASE – DRAM_BASE+4MB to make kernel memory i
- user mode cannot access pages here unless the PTE U is set, so user mode can't see here. 

realistically here, there are only 2 entries in the l1 page table: the 4mb user space, and the 4mb kernel space.
- the full L1 table covers the entire 32-bit virtual addres space and has 1024 entries
- hweover, only 2 entries are initialized: l1pt\[0\] (user reigion) and l1pt\[PTES_PER_PT - 1\] (kernel region). 
- the rest of the entries are invalid. l1pt has 1024 entries, but only two are populated: one for user space and one for kernel space.

say I have a pointer containing an address of a kernel variable 
- when i dereference that pionter, the CPU will see the virtual address and thne access the physical memory by walking the page table

```
  uintptr_t vm_choice = SATP_MODE_CHOICE;
  uintptr_t satp_value = ((uintptr_t)l1pt >> PGSHIFT)
                        | (vm_choice * (SATP_MODE & ~(SATP_MODE<<1)));
  write_csr(satp, satp_value);
```
- loads the physical page number of l1pt into SATP, sets mode, and enables mmu 
- satp: supervisor address translation and protection register. contrls paging: which paging scheme (RV32), ASID (address space ID for process isolation???) , PPN (physical page number of the L1 page table)

why is the ppn of the l1 page table needed? 
- the CPU needs to knwo where hte L1 page table is in physical mem in order to index into it
- doesn't ned the ful address (just the ppn) because wejust need to index into it, and the l1 page itself is 4kb (exactly 1 page in size)

after this:
- pointers must now be virtual addresses -- if i derefence a physical address direclty, it won't point to what I expect
- unmapped virtual addresses will cause faults
- page tables will tkae over address translation

```
#define flush_page(addr) asm volatile ("sfence.vma %0" : : "r" (addr) : "memory")

flush_page(DRAM_BASE);
```
flushes TLB. a TLB is: 
- caches recently-used virtual -> physical translations in the translation lookaside buffer
- store: Virtual Address → Physical Address + Permissions
- if page table changes, teh TLB may contian stale mappings

sfense.vma
- supervsor fence for VM acess
- tells the CPU to invalidate TLB entries for the virtual addr (or all if none specified)

`sfence.vma` at DRAM_BASE
- we establisehd new stuff at DRAM_BASE so we want to invalidate the TLB endry for that addr
- i can't figure out exaclty why -- my question:

```
in vm_boot, the DRAM_BASE addr is flushed right after setting up the initial page table stuff here.

What's the benefit of flushing, if we'd just written to satp to enable the MMU? My understanding is likely wrong lol, but I'm thinking that because the MMU wasn't active before (?) to populate the TLB, it's probably good practice to flush, but we don't really need to. But then that also raises the question as to why not flush everything instead of just DRAM_BASE
```

answer to be answered lol

however, the significance of dram_base is that we were using the physical addresses here before to strstraight up run the progrlam. this is where `text.init` is located.

Yup, it's that:
- DRAM_BASE + above has the actual instructions that are executing
- so even tho we literally just activated the mmu, its already adding the stuff to the TLB

I also may have found an unsafety thing: 
- "how does that not get in the way of the instructions executing though? if we switched from physical -> virtual memory and then have a small set of instructions inbetween before it gets flusehd
- "from will: more like before the satp csr write is committed, the physical address still applies. so after the satp write completes, everything is virtualized, but there will be two instructions in the pipeline after it. kind of ugly
- the pipeline saves the couple of instrs just after, already fetched. its not sound. but i cannot remember if the pipeline prevents this from happening at all. I think this is covered by the mem-use hazard we have setup?
- solution: push in a couple of no-ops or sum

now: 
```
pmp setup stuff
```
- pmp is physical memory protection
- in RISC-V, it's a hardware feature that lets machien mode define which memory regions can be accessed 
- can be used to do stuff like protect MIO regions and restrict supervisor

flow:
```
Virtual address
   ↓ (MMU / page table)
Physical address
   ↓ (PMP check)
Memory access allowed or denied
```

currently, full access with this :

```
  // Set up PMPs if present, ignoring illegal instruction trap if not.
  uintptr_t pmpc = PMP_NAPOT | PMP_R | PMP_W | PMP_X;
  uintptr_t pmpa = ((uintptr_t)1 << (__riscv_xlen == 32 ? 31 : 53)) - 1;
  asm volatile ("la t0, 1f\n\t"
                "csrrw t0, mtvec, t0\n\t"
                "csrw pmpaddr0, %1\n\t"
                "csrw pmpcfg0, %0\n\t"
                ".align 2\n\t"
                "1: csrw mtvec, t0"
                : : "r" (pmpc), "r" (pmpa) : "t0");
```
- idrk what this does
- however, it apparently disableds restrictions

## Raw Meeting Notes

Tohost

RVB <-> TestBench 

Signal connected between teh test bench and teh actual core itself 
- One of them being the address, wdata…

Within the RVB’s address space, there’s something called tohost
- Its an address specifying we want to write something to the test bench
- Tohost currently is cacheable which s why they have to fence it


Test bench
- At all points in time, monitors what the core does
- Any address that it reads
- If a spa

Test bench
- $RISCV_CORE: path to ventilator build

Cole ..
- Reads the elf of the specified path to the test bench in riscvtests, and then the takes the those region within the  riscvtests and u read it in run_test.py

When u build the riscv processor, it will create an executable with vtop_core, and this is what u use to run the inary that you’re tryna run. 

- In Rvb, tb_core.cc

Register 28 or sumn with test bench

Within the verification tests. Start.S jumps to done, and loads the value of flag into x28!! 
- In tb_core.cc: in the main loop, checks rte core, program runs in the while loop…. It verifies correctness, and if we are not using those && dut.top_core line 424
- There’s two things here : the first one is using those —- specifying that it is a test that RISCV corp has written, and reading the those address — whatever is defined within the test for that elf —> if its .. see if x28 is 

Non-cacheable region
- Anything from 0XF0000 _> onwards is non-cacheable 

How does print work within the simulation 
- Cole’s method: writing directly to 0XFFFFFFFC within the verificication tests 
- #define MAGIC_ADDR in utility.h
- In test bench we have the same magic address defined 
- 153 line 

How do we build a cc file into system verilog? 
- Include the ventilator files 
- vtop_core.h create a class — essentially — for our module — so when we define our class, we just give it a DUT pointer, and is just a pointer to a class where (dut_ptr) where we can control the ptr, and those r all inputs and outputs for the core 
- Converts system verilog to cpp so that it runs 

Mie 
- Enable interrupts for whatever interrupts it specifies
MIP
- For pending interrupts 

MIP
- Different bits, and each goes to sumn (machine timer, machine software…) and it does htehse for hte other modes.. they can alternate
- The spec defines priority 

Nested interrupts
- We can take nested interrupts cz we can offload registers onto the stack, but that’s not set up anywhere 

Say we r in a s-mode interrupt handler and a m-mode interrupt happens; 
- priv_in_ex_handler.sv gives priority handling stuff 
- In line 32 — m-mode interrupt take sprioirty over what’s going on currently 

Trap handler
- Between the time it takes for the trap handler to push everything onto the stack and it actually df

trap_entry
- Puts sumn onto the stack
- Puts all the store words
- Jumps to handle_trap 
- Goes to different handle_trap
- Looks at hte cause of the trap

In the riscv corporation traps, they dont have the __attribute__
- They do sumn stufff with the scratch registers…. 

Do it in the riscv corporation way 
- We wont have to rely on the compiler to do it for us

So recomended flow: 
1. trap_entry
2. J handle_trap
3. 


